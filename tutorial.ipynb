{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark tutorial\n",
    "\n",
    "This notebook provides basic tutorial of benchmark of bearing fault diagnosis model and optimizer's hyperparameter. Core implementation of this code is in the `fdob` module. This modlue provides data download, data preprocessing, model implementation, quasi-random hyperparameter sampling, and model trainning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fdob\n",
    "import fdob.processing as processing\n",
    "import fdob.model as model\n",
    "import info\n",
    "import benchmark\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download\n",
    "\n",
    "We can download the CWRU and MFPT datasets using `download_cwru` and `download_mfpt`, respectively. These functions automatically download each dataset from URLs and return pandas `DataFrame`. `split_dataframe` splits dataframe to train, validation, and test `Dataframe`. `build_from_dataframe` build `numpy.ndarray` dataset by overlapping. In this tutorial, we use the CWRU dataset for training, and the data is generated with the sample length 4,096 and shift size 2,048."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fdob.download_cwru(\"./data/cwru\")\n",
    "\n",
    "# We exclude label named 999 and 0 HP motor load condition.\n",
    "df = df[(df[\"label\"] != 999) & (df[\"load\"] != 0)]\n",
    "\n",
    "train_df, val_df, test_df = fdob.split_dataframe(df, 0.6, 0.2)\n",
    "\n",
    "X_train, y_train = fdob.build_from_dataframe(train_df, 4096, 2048, False)\n",
    "X_val, y_val = fdob.build_from_dataframe(val_df, 4096, 2048, False)\n",
    "X_test, y_test = fdob.build_from_dataframe(test_df, 4096, 2048, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the model and preparing `DataLoader`\n",
    "\n",
    "The models and the initial hyperparameter search spaces for each optimizer used in the paper are in the `info.py` file. `info.model` contains the model, input length of the model, and `transform` of data. `info.hparam` contains the information of the search space of four optimizers, sgd, momentum, RMSProp, and adam. Users can employ the models and hyperparameter search space in the `info.py` but also can use the custom models and search space.\n",
    "\n",
    "To train the model using PyTorch Lightning, `DataLoader` should be prepared. We provide `DatasetHandler`, which is the collection of multi-domain datasets. `assign` method generates `DataLoader`, and users can access the `DataLoader` by key of `DataLoader`. This tutorial uses two `DataLoader`s. `DataLoader` with key `cwru` is the noise-free data from the CWRU dataset, and `DataLoader` with key `cwru0` is the noisy data generated by Gaussian noise SNR 0dB from the CWRU dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"wdcnn\"\n",
    "\n",
    "model = info.model[model_name][\"model\"]\n",
    "sample_length = info.model[model_name][\"sample_length\"]\n",
    "tf_data = info.model[model_name][\"tf\"]\n",
    "tf_label = [processing.NpToTensor()]\n",
    "batch_size = 32\n",
    "num_workers = 1\n",
    "\n",
    "dmodule = fdob.DatasetHandler()\n",
    "\n",
    "dmodule.assign(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    sample_length,\n",
    "    \"cwru\",\n",
    "    transforms.Compose(tf_data),\n",
    "    transforms.Compose(tf_label),\n",
    "    batch_size,\n",
    "    num_workers\n",
    ")\n",
    "\n",
    "dmodule.assign(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    sample_length,\n",
    "    \"cwru0\",\n",
    "    transforms.Compose([processing.AWGN(0)] + tf_data),\n",
    "    transforms.Compose(tf_label),\n",
    "    batch_size,\n",
    "    num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access to the DataLoader of train dataset of the noise-free CWRU dataset.\n",
    "dmodule.dataloaders[\"cwru\"][\"train\"]\n",
    "\n",
    "# Access to the Dataset of train dataset of the noisy CWRU dataset.\n",
    "dmodule.dataloaders[\"cwru\"][\"train\"].dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter sampling\n",
    "\n",
    "`log_qsample` samples hyperaprameters in log-scale quasi-random distribution. For example, if adam optimizer and hyperparameter search space defined in `info.py` is used, each hyperparameters are sampled from probability distributions below.\n",
    "\n",
    "* $\\eta \\sim 10^{U[-4, -1]}$\n",
    "* $1 - \\beta_{1} \\sim 10^{U[-3, 0]}$\n",
    "* $1 - \\beta_{2} \\sim 10^{U[-4, -1]}$\n",
    "* $\\epsilon \\sim 10^{U[-10, 0]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_exps = 4\n",
    "\n",
    "hparam_info = info.hparam[\"adam\"]\n",
    "\n",
    "hparams = fdob.log_qsample(\n",
    "    hparam_info[\"n_params\"],\n",
    "    hparam_info[\"param_names\"],\n",
    "    hparam_info[\"lb\"],\n",
    "    hparam_info[\"ub\"],\n",
    "    hparam_info[\"reversed\"],\n",
    "    n_exps\n",
    ")\n",
    "\n",
    "hparams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and evaluation\n",
    "\n",
    "To benchmark, following materials should be prepared. \n",
    "\n",
    "* train `DataLoader`\n",
    "* validation `DataLoader`\n",
    "* PyTorch model\n",
    "* model's keyword argument (if there is no keyword argument, pass `None`)\n",
    "* PyTorch optimizer from `torch.optim`\n",
    "* optimizer's keyword argument (if there is no keyword argument, pass `None`)\n",
    "* PyTorch loss function from `torch.nn`\n",
    "* loss function's keyword argument (if there is no keyword argument, pass `None`)\n",
    "* The number of epochs\n",
    "* Random seed (if `None` is passed, random seed is not set)\n",
    "* The number of GPU (only CUDA GPU is supported)\n",
    "* Result directory of the experiemnt\n",
    "\n",
    "Following code train the WDCNN using the first hyperparameter determined above, and the result is saved in the `./logs/mytest`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    \"n_classes\": 10\n",
    "}\n",
    "\n",
    "opt = hparam_info[\"optimizer\"]\n",
    "opt_kwargs = {\n",
    "    \"lr\": hparams[\"lr\"][0],\n",
    "    \"betas\": (hparams[\"beta1\"][0], hparams[\"beta2\"][0]),\n",
    "    \"eps\": hparams[\"eps\"][0]\n",
    "}\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss\n",
    "loss_kwargs = None\n",
    "\n",
    "seed =6464\n",
    "n_gpu = 0\n",
    "n_epochs = 5\n",
    "\n",
    "result_dir = \"./logs/mytest\"\n",
    "\n",
    "benchmark.train(\n",
    "    dmodule.dataloaders[\"cwru\"][\"train\"],\n",
    "    dmodule.dataloaders[\"cwru\"][\"val\"],\n",
    "    model,\n",
    "    model_kwargs,\n",
    "    opt,\n",
    "    opt_kwargs,\n",
    "    loss,\n",
    "    loss_kwargs,\n",
    "    n_epochs,\n",
    "    seed,\n",
    "    n_gpu,\n",
    "    result_dir\n",
    ")\n",
    "benchmark.test(\n",
    "    dmodule.dataloaders[\"cwru\"][\"test\"],\n",
    "    model,\n",
    "    model_kwargs,\n",
    "    opt,\n",
    "    opt_kwargs,\n",
    "    loss,\n",
    "    loss_kwargs,\n",
    "    n_epochs,\n",
    "    seed,\n",
    "    n_gpu,\n",
    "    result_dir,\n",
    "    \"noise-free\"\n",
    ")\n",
    "benchmark.test(\n",
    "    dmodule.dataloaders[\"cwru0\"][\"test\"],\n",
    "    model,\n",
    "    model_kwargs,\n",
    "    opt,\n",
    "    opt_kwargs,\n",
    "    loss,\n",
    "    loss_kwargs,\n",
    "    n_epochs,\n",
    "    seed,\n",
    "    n_gpu,\n",
    "    result_dir,\n",
    "    \"noise\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
